{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T16:23:02.216031Z",
     "start_time": "2025-02-27T16:23:01.975317Z"
    }
   },
   "source": [
    "import stores\n",
    "import numpy as np\n",
    "from pympler import asizeof \n",
    "\n",
    "from performance_helper import performance_gen, performance_gen_leveled"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:24:27.530411Z",
     "start_time": "2025-02-27T16:23:31.353946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 8\n",
    "with open('tsne_transform/scalingData/random_3d_100000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "789298cd17b76224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  18.170623302459717\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  27.59845471382141\n",
      "All tasks completed\n",
      "Time to create stores:  7.872917413711548\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  35.48813986778259\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  295.3200186654223\n",
      "Max:  506\n",
      "Min:  63\n",
      "Size of cloud storage 79880624\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  158.59546517905343\n",
      "Max:  259\n",
      "Min:  32\n",
      "Size of leveled cloud storage 36925744\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "38104.0 1.124202905764985 1000\n",
      "38.104 1.1242029057649843 1000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:25:30.349904Z",
     "start_time": "2025-02-27T16:24:27.541899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 6\n",
    "with open('tsne_transform/scalingData/random_3d_200000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "c9ce04119ee4116",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  20.5873761177063\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  29.9948570728302\n",
      "All tasks completed\n",
      "Time to create stores:  8.176082372665405\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  38.188676595687866\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  298.6885886651372\n",
      "Max:  519\n",
      "Min:  26\n",
      "Size of cloud storage 88958584\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  159.82390494768575\n",
      "Max:  270\n",
      "Min:  59\n",
      "Size of leveled cloud storage 37743360\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "36797.0 1.1716029540337873 1000\n",
      "36.797 1.1716029540337862 1000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:27:12.558674Z",
     "start_time": "2025-02-27T16:25:30.404540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 160\n",
    "gran_result = 6\n",
    "with open('tsne_transform/scalingData/random_3d_400000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "b9745c54d9f20082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  44.87604212760925\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  40.97270679473877\n",
      "All tasks completed\n",
      "Time to create stores:  8.974834442138672\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  49.96399211883545\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  560.6489972100858\n",
      "Max:  900\n",
      "Min:  91\n",
      "Size of cloud storage 262664056\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  189.79226200032107\n",
      "Max:  291\n",
      "Min:  70\n",
      "Size of leveled cloud storage 49128440\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "38861.0 1.1210139108574053 1000\n",
      "38.861 1.1210139108574058 1000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:28:35.821646Z",
     "start_time": "2025-02-27T16:27:12.603738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 4\n",
    "with open('tsne_transform/scalingData/random_3d_800000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "bdf28cb3af96b00e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  30.880019903182983\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  28.793726205825806\n",
      "All tasks completed\n",
      "Time to create stores:  8.434617280960083\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  37.2450168132782\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  473.3875826290687\n",
      "Max:  849\n",
      "Min:  25\n",
      "Size of cloud storage 140431960\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  167.77118502423534\n",
      "Max:  279\n",
      "Min:  32\n",
      "Size of leveled cloud storage 41929792\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "36240.0 1.1786785217327556 999\n",
      "36.274274274274276 1.177011082124855 999\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:33:51.626949Z",
     "start_time": "2025-02-27T16:28:35.869862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 280\n",
    "gran_result = 3\n",
    "with open('tsne_transform/scalingData/random_3d_1600000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "2985bedaa6738c98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  143.88382411003113\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  126.3322765827179\n",
      "All tasks completed\n",
      "Time to create stores:  11.628422260284424\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  137.98525595664978\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  529.8776793270808\n",
      "Max:  946\n",
      "Min:  67\n",
      "Size of cloud storage 758586888\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  318.96443791329904\n",
      "Max:  514\n",
      "Min:  93\n",
      "Size of leveled cloud storage 88804768\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "39556.0 1.1218406649291914 1000\n",
      "39.556 1.1218406649291917 1000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:44:41.784207Z",
     "start_time": "2025-02-27T16:33:51.689344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 400\n",
    "gran_result = 2\n",
    "with open('tsne_transform/scalingData/random_3d_3200000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "7b7d0a71634c4c87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  280.87386989593506\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  280.3412365913391\n",
      "All tasks completed\n",
      "Time to create stores:  15.572172403335571\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  295.93449568748474\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  456.07431443930324\n",
      "Max:  889\n",
      "Min:  29\n",
      "Size of cloud storage 1332088224\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  532.4652034261242\n",
      "Max:  864\n",
      "Min:  139\n",
      "Size of leveled cloud storage 161291264\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "40168.0 1.1063106519458383 1000\n",
      "40.168 1.1063106519458383 1000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:44:51.907418Z",
     "start_time": "2025-02-27T16:44:41.965155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 35\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_3d_10000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "db13586bb4463243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  1.4822781085968018\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  2.4023923873901367\n",
      "All tasks completed\n",
      "Time to create stores:  4.4095587730407715\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  6.82073974609375\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  222.80448717948718\n",
      "Max:  399\n",
      "Min:  54\n",
      "Size of cloud storage 5360104\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  86.02024974687816\n",
      "Max:  122\n",
      "Min:  34\n",
      "Size of leveled cloud storage 11077504\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "38144.0 1.1173031127295372 1000\n",
      "38.144 1.1173031127295363 1000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:45:01.184331Z",
     "start_time": "2025-02-27T16:44:51.916357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 35\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_3d_20000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "44bbc1f6f3f3288f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  1.6496989727020264\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  1.79854154586792\n",
      "All tasks completed\n",
      "Time to create stores:  4.455390214920044\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  6.264148712158203\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  278.7790821771612\n",
      "Max:  491\n",
      "Min:  54\n",
      "Size of cloud storage 6624824\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  89.54108216432866\n",
      "Max:  129\n",
      "Min:  39\n",
      "Size of leveled cloud storage 11613512\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "37733.0 1.1421350617982229 1000\n",
      "37.733 1.1421350617982233 1000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:45:15.916660Z",
     "start_time": "2025-02-27T16:45:01.204815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 50\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_3d_40000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "3d948d383a5c1b74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  3.515519380569458\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  3.651103973388672\n",
      "All tasks completed\n",
      "Time to create stores:  5.631211996078491\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  9.29600715637207\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  386.68356456776945\n",
      "Max:  641\n",
      "Min:  81\n",
      "Size of cloud storage 18102080\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  103.89195855944746\n",
      "Max:  161\n",
      "Min:  48\n",
      "Size of leveled cloud storage 18000072\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "38372.0 1.1414162234427225 1000\n",
      "38.372 1.141416223442722 1000\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:57:05.922574Z",
     "start_time": "2024-10-09T14:57:05.921101Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ed851a55d830b8d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
