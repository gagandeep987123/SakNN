{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-28T22:21:40.361540Z",
     "start_time": "2025-02-28T22:21:40.223297Z"
    }
   },
   "source": [
    "import stores\n",
    "import numpy as np\n",
    "from pympler import asizeof \n",
    "\n",
    "from performance_helper import performance_gen, performance_gen_leveled"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:22:22.371755Z",
     "start_time": "2025-02-28T22:21:40.618268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 8\n",
    "with open('tsne_transform/scalingData/random_2d_100000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "789298cd17b76224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  14.634905576705933\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  18.02961254119873\n",
      "All tasks completed\n",
      "Time to create stores:  6.699389696121216\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  24.744054794311523\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  255.35357409182913\n",
      "Max:  390\n",
      "Min:  66\n",
      "Size of local storage 300528\n",
      "Size of cloud storage 41801024\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  134.58914146158182\n",
      "Max:  200\n",
      "Min:  42\n",
      "Size of local storage 170480\n",
      "Size of leveled cloud storage 24393040\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49709.0 1.0011661189265917 1000\n",
      "49.709 1.0011661189265912 1000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:23:01.168792Z",
     "start_time": "2025-02-28T22:22:22.386748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 8\n",
    "with open('tsne_transform/scalingData/random_2d_100000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])"
   ],
   "id": "bc976c8a503e4d63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  14.76752257347107\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  18.335150241851807\n",
      "All tasks completed\n",
      "Time to create stores:  5.638665437698364\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  23.98777484893799\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:23:01.453285Z",
     "start_time": "2025-02-28T22:23:01.195129Z"
    }
   },
   "cell_type": "code",
   "source": "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True, timeCalFlag=True)",
   "id": "d26df05585eac98a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.24819636344909668\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:23:01.721199Z",
     "start_time": "2025-02-28T22:23:01.491601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, query_sizes1, query_sizes2, result_sizes1, result_sizes2 = (\n",
    "    performance_gen_leveled(\n",
    "        df, result_store, result_store_enc,\n",
    "        query_store_level1=leveled_stores[2],\n",
    "        result_store_level1=leveled_stores[0],\n",
    "        result_store_level1_enc=leveled_stores[1],\n",
    "        isEnc=True, timeCalFlag=True\n",
    "    )\n",
    ")"
   ],
   "id": "427e82646272b30a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.22017264366149902\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:23:49.485770Z",
     "start_time": "2025-02-28T22:23:01.732371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 6\n",
    "with open('tsne_transform/scalingData/random_2d_200000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "c9ce04119ee4116",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  15.5989830493927\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  22.71291446685791\n",
      "All tasks completed\n",
      "Time to create stores:  6.051924467086792\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  28.78060293197632\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  258.9036298802231\n",
      "Max:  372\n",
      "Min:  47\n",
      "Size of local storage 350128\n",
      "Size of cloud storage 49570808\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  140.11256544502618\n",
      "Max:  206\n",
      "Min:  78\n",
      "Size of local storage 183504\n",
      "Size of leveled cloud storage 27407088\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49455.0 1.0038372414528942 1000\n",
      "49.455 1.0038372414528942 1000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:25:05.577684Z",
     "start_time": "2025-02-28T22:23:49.508655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 160\n",
    "gran_result = 6\n",
    "with open('tsne_transform/scalingData/random_2d_400000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "b9745c54d9f20082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  31.807405710220337\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  31.360419750213623\n",
      "All tasks completed\n",
      "Time to create stores:  6.402171611785889\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  37.777318716049194\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  494.5076512023318\n",
      "Max:  695\n",
      "Min:  119\n",
      "Size of local storage 527120\n",
      "Size of cloud storage 136419992\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  161.18996350364964\n",
      "Max:  227\n",
      "Min:  85\n",
      "Size of local storage 175504\n",
      "Size of leveled cloud storage 29920320\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49967.0 1.0001705951775053 1000\n",
      "49.967 1.0001705951775053 1000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:26:15.889107Z",
     "start_time": "2025-02-28T22:25:05.632504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 125\n",
    "gran_result = 4\n",
    "with open('tsne_transform/scalingData/random_2d_800000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "bdf28cb3af96b00e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  25.775943517684937\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  24.770424604415894\n",
      "All tasks completed\n",
      "Time to create stores:  5.761129379272461\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  30.54779291152954\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  422.99439799331105\n",
      "Max:  629\n",
      "Min:  31\n",
      "Size of local storage 382864\n",
      "Size of cloud storage 85552680\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  135.15097556355371\n",
      "Max:  193\n",
      "Min:  73\n",
      "Size of local storage 169072\n",
      "Size of leveled cloud storage 24287536\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49683.0 1.0044917918642313 1000\n",
      "49.683 1.0044917918642315 1000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:31:01.473587Z",
     "start_time": "2025-02-28T22:26:15.940423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 280\n",
    "gran_result = 3\n",
    "with open('tsne_transform/scalingData/random_2d_1600000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "2985bedaa6738c98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  130.40115404129028\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  112.33242082595825\n",
      "All tasks completed\n",
      "Time to create stores:  10.419866561889648\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  122.77268671989441\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  475.7804529000758\n",
      "Max:  723\n",
      "Min:  101\n",
      "Size of local storage 1900784\n",
      "Size of cloud storage 474721928\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  301.3179603767457\n",
      "Max:  423\n",
      "Min:  138\n",
      "Size of local storage 197200\n",
      "Size of leveled cloud storage 61198704\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49972.0 1.0001515705297532 1000\n",
      "49.972 1.0001515705297532 1000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:35:26.270973Z",
     "start_time": "2025-02-28T22:31:01.527416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 280\n",
    "gran_result = 3\n",
    "with open('tsne_transform/scalingData/random_2d_1600000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])"
   ],
   "id": "9036827fe2160338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  139.42741107940674\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  114.42975354194641\n",
      "All tasks completed\n",
      "Time to create stores:  10.69023871421814\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  125.14129972457886\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:35:27.793020Z",
     "start_time": "2025-02-28T22:35:26.340859Z"
    }
   },
   "cell_type": "code",
   "source": "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True, timeCalFlag=True)",
   "id": "64789d73223a84f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.3046000003814697\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:35:28.275665Z",
     "start_time": "2025-02-28T22:35:27.861295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40,query_sizes1, query_sizes2, result_sizes1, result_sizes2  = (\n",
    "    performance_gen_leveled(\n",
    "        df, result_store, result_store_enc,\n",
    "        query_store_level1=leveled_stores[2],\n",
    "        result_store_level1=leveled_stores[0],\n",
    "        result_store_level1_enc=leveled_stores[1],\n",
    "        isEnc=True, timeCalFlag=True\n",
    "    )\n",
    ")"
   ],
   "id": "c1efa505815da679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.2752096652984619\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:45:42.009888Z",
     "start_time": "2025-02-28T22:35:28.345078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 400\n",
    "gran_result = 2\n",
    "with open('tsne_transform/scalingData/random_2d_3200000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "7b7d0a71634c4c87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  288.6937201023102\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  238.91652703285217\n",
      "All tasks completed\n",
      "Time to create stores:  14.7098548412323\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  253.64584803581238\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  413.85698639704134\n",
      "Max:  669\n",
      "Min:  36\n",
      "Size of local storage 4032208\n",
      "Size of cloud storage 881928512\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  493.8533054767945\n",
      "Max:  672\n",
      "Min:  217\n",
      "Size of local storage 214576\n",
      "Size of leveled cloud storage 107854808\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49935.0 1.0003334002055217 1000\n",
      "49.935 1.000333400205522 1000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:32.759622Z",
     "start_time": "2025-02-28T22:45:42.131188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 400\n",
    "gran_result = 2\n",
    "with open('tsne_transform/scalingData/random_2d_3200000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])"
   ],
   "id": "6214c9d6f050f5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  281.7515435218811\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  235.6429204940796\n",
      "All tasks completed\n",
      "Time to create stores:  12.830763816833496\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  248.4891836643219\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:35.915110Z",
     "start_time": "2025-02-28T22:54:32.872949Z"
    }
   },
   "cell_type": "code",
   "source": "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True, timeCalFlag=True)",
   "id": "aea40e85f385eabf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.7383475303649902\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:36.551768Z",
     "start_time": "2025-02-28T22:54:36.007632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40,query_sizes1, query_sizes2, result_sizes1, result_sizes2  = (\n",
    "    performance_gen_leveled(\n",
    "        df, result_store, result_store_enc,\n",
    "        query_store_level1=leveled_stores[2],\n",
    "        result_store_level1=leveled_stores[0],\n",
    "        result_store_level1_enc=leveled_stores[1],\n",
    "        isEnc=True, timeCalFlag=True\n",
    "    )\n",
    ")"
   ],
   "id": "24f19e6de6a536f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.29708051681518555\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:44.224847Z",
     "start_time": "2025-02-28T22:54:36.646574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 35\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_2d_10000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "db13586bb4463243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  1.2930996417999268\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  1.84446382522583\n",
      "All tasks completed\n",
      "Time to create stores:  2.9444360733032227\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  4.800606966018677\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  223.0632258064516\n",
      "Max:  348\n",
      "Min:  56\n",
      "Size of local storage 24944\n",
      "Size of cloud storage 3063336\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  55.25364530349271\n",
      "Max:  74\n",
      "Min:  28\n",
      "Size of local storage 94512\n",
      "Size of leveled cloud storage 6093088\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49575.0 1.0026128999920447 1000\n",
      "49.575 1.0026128999920445 1000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:44.249935Z",
     "start_time": "2025-02-28T22:54:44.247835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gran_query = 35\n",
    "# gran_result = 13\n",
    "# with open('tsne_transform/scalingData/random_2d_10000_scikit.npy', 'rb') as f:\n",
    "#      embedding = np.load(f)\n",
    "#      df = np.load(f)\n",
    "# result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n"
   ],
   "id": "2d7082cbfb9ac384",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:44.355328Z",
     "start_time": "2025-02-28T22:54:44.272705Z"
    }
   },
   "cell_type": "code",
   "source": "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True, timeCalFlag=True)",
   "id": "cbd7c0c0e74531e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0786592960357666\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:44.542643Z",
     "start_time": "2025-02-28T22:54:44.371660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40,query_sizes1, query_sizes2, result_sizes1, result_sizes2  = (\n",
    "    performance_gen_leveled(\n",
    "        df, result_store, result_store_enc,\n",
    "        query_store_level1=leveled_stores[2],\n",
    "        result_store_level1=leveled_stores[0],\n",
    "        result_store_level1_enc=leveled_stores[1],\n",
    "        isEnc=True, timeCalFlag=True\n",
    "    )\n",
    ")"
   ],
   "id": "8be662cf2e7b7fb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.16750693321228027\n",
      "All Tasks done\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:54:54.433570Z",
     "start_time": "2025-02-28T22:54:44.575619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 35\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_2d_20000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "44bbc1f6f3f3288f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  1.6937627792358398\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  2.677889347076416\n",
      "All tasks completed\n",
      "Time to create stores:  4.089815616607666\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  6.783358573913574\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  279.76132075471696\n",
      "Max:  454\n",
      "Min:  86\n",
      "Size of local storage 34064\n",
      "Size of cloud storage 5137864\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  63.339161809603105\n",
      "Max:  88\n",
      "Min:  35\n",
      "Size of local storage 115440\n",
      "Size of leveled cloud storage 8343856\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49630.0 1.0017080787626689 1000\n",
      "49.63 1.0017080787626689 1000\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T22:55:07.557583Z",
     "start_time": "2025-02-28T22:54:54.440968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gran_query = 50\n",
    "gran_result = 13\n",
    "with open('tsne_transform/scalingData/random_2d_40000_scikit.npy', 'rb') as f:\n",
    "     embedding = np.load(f)\n",
    "     df = np.load(f)\n",
    "result_store, result_store_enc, query_store, leveled_stores = stores.stores_gen_parallel(org_data=df, tsne_red=embedding, div_qrs=gran_query, gran_ris=gran_result, leveled_gen=True, leveldParams=[100, 10])\n",
    "\n",
    "print(\"#Stats about result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in result_store:\n",
    "    store_lens.append(result_store[s].shape[0])\n",
    "    max_size = max(max_size, result_store[s].shape[0])\n",
    "    min_size = min(min_size, result_store[s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(query_store))\n",
    "print(\"Size of cloud storage\",asizeof.asizeof(result_store_enc))\n",
    "\n",
    "print(\"\\n\\n#Stats about leveled result stores\")\n",
    "store_lens = []\n",
    "max_size = 1\n",
    "min_size = 1000000000\n",
    "for s in leveled_stores[0]:\n",
    "    store_lens.append(leveled_stores[0][s].shape[0])\n",
    "    max_size = max(max_size, leveled_stores[0][s].shape[0])\n",
    "    min_size = min(min_size, leveled_stores[0][s].shape[0])\n",
    "# print(sum(store_lens))\n",
    "# print(len(store_lens))\n",
    "print(\"Avg result store entry size: \",sum(store_lens)/len(store_lens))\n",
    "print(\"Max: \", max_size)\n",
    "print(\"Min: \", min_size)\n",
    "print(\"Size of local storage\",asizeof.asizeof(leveled_stores[2]))\n",
    "print(\"Size of leveled cloud storage\",asizeof.asizeof(leveled_stores[0]))\n",
    "\n",
    "oars50, recalls50, flags, recalls5, recalls10, recalls20, recalls30, recalls40, oars5, oars10, oars20, oars30, oars40, result_sizes, query_sizes = performance_gen(df, query_store, result_store, result_store_enc, isEnc=True)\n",
    "\n",
    "print(\"\\n\\n###Printing for 50###\")\n",
    "print(sum(recalls50), np.average(oars50), np.sum(flags))\n",
    "indices = np.where(np.array(flags) == True)[0]\n",
    "print(sum(recalls50[i] for i in indices) / len(indices), sum(oars50[i] for i in indices) / len(indices), len(indices))"
   ],
   "id": "3d948d383a5c1b74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n",
      "Time to create stores:  2.881906270980835\n",
      "Tsne processing\n",
      "Tsne processing Done\n",
      "Tsne processing time:  3.408372402191162\n",
      "All tasks completed\n",
      "Time to create stores:  4.90179967880249\n",
      "Result Store level1 Done\n",
      "Time to create leveled stores:  8.32537317276001\n",
      "#Stats about result stores\n",
      "Avg result store entry size:  373.3884992987377\n",
      "Max:  549\n",
      "Min:  70\n",
      "Size of local storage 45776\n",
      "Size of cloud storage 9072176\n",
      "\n",
      "\n",
      "#Stats about leveled result stores\n",
      "Avg result store entry size:  68.47379865188084\n",
      "Max:  98\n",
      "Min:  42\n",
      "Size of local storage 147312\n",
      "Size of leveled cloud storage 11365264\n",
      "All Tasks done\n",
      "\n",
      "\n",
      "###Printing for 50###\n",
      "49915.0 1.0004426866310991 1000\n",
      "49.915 1.0004426866310994 1000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:57:05.922574Z",
     "start_time": "2024-10-09T14:57:05.921101Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ed851a55d830b8d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
